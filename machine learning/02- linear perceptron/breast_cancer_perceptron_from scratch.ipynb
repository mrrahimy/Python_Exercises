{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "x = data.data\n",
    "y = data.target\n",
    "features = data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (f'feature:{features}\\nx:{x}\\ny:{y}')\n",
    "# df = pd.DataFrame(x, columns=features)\n",
    "# df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_changer(y):\n",
    "    return np.where(y==0, -1, 1) # change all elements of y equal to 0 to new value -1 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class perceptron:\n",
    "    def __init__(self, epoch=1000, learning_rate=0.01):\n",
    "        self.epoch = epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.w = None\n",
    "        self.misclass =[]\n",
    "        self.accuracy = 0\n",
    "\n",
    "        self.TP = None\n",
    "        self.TN = None\n",
    "        self.FP = None\n",
    "        self.FN = None\n",
    "\n",
    "    def xw_mapper (self, x):\n",
    "        return np.where(x>=0 , 1 , -1)\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        n, m = x.shape #samples, features\n",
    "        self.w = np.zeros(m+1)\n",
    "        x = np.c_[np.ones(n), x]\n",
    "        for ep in range(self.epoch):\n",
    "            num_misclassified = 0\n",
    "            for i in range(n):\n",
    "                xw =  x[i] @ self.w\n",
    "                y_hat = self.xw_mapper(xw)\n",
    "                if y_hat != y[i]:                    \n",
    "                    self.w += self.learning_rate * y[i] * x[i]\n",
    "                    num_misclassified += 1\n",
    "            self.misclass.append(num_misclassified)\n",
    "            if num_misclassified ==0:\n",
    "                break\n",
    "        self.accuracy = 1-num_misclassified/n\n",
    "    def pred(self, x):\n",
    "        x = np.c_[np.ones(x.shape[0]), x]\n",
    "        return self.xw_mapper(x @ self.w)\n",
    "    def metric(self, y_hat, y):\n",
    "        self.TP = np.sum((y == 1) & (y_hat == 1))  # True Positive\n",
    "        self.TN = np.sum((y == -1) & (y_hat == -1))  # True Negative\n",
    "        self.FP = np.sum((y == -1) & (y_hat == 1))  # False Positive\n",
    "        self.FN = np.sum((y == 1) & (y_hat == -1))  # False Negative\n",
    "    def Accuracy_t (self):\n",
    "        return (self.TP + self.TN) / (self.TP + self.TN + self.FP + self.FN)\n",
    "    def Precision (self):\n",
    "        return self.TP / (self.TP + self.FP) if (self.TP + self.FP) > 0 else 0\n",
    "    def Recall (self):\n",
    "        return self.TP / (self.TP + self.FN) if (self.TP + self.FN) > 0 else 0\n",
    "    def F1_score(self):\n",
    "        return  2 * (self.Precision() * self.Recall()) / (self.Precision() + self.Recall()) if (self.Precision() + self.Recall()) > 0 else 0\n",
    "    def Specificity (self):\n",
    "        return self.TN / (self.TN + self.FP) if (self.TN + self.FP) > 0 else 0\n",
    "    def NPV (self):\n",
    "        return self.TN / (self.TN + self.FN) if (self.TN + self.FN) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_changer(y)\n",
    "x_train, x_test, y_train, y_test = train_test_split (x, y, test_size = 0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \n",
       "\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}} \\qquad \\text{when train data is symetric} \n",
       "\\\\ \\text{ }\n",
       "\\\\ \\text{ }\n",
       "\\\\ \\text{> when we know the model's prediction (positive) or (negative), NPV and precision are used}\n",
       "\\\\ \\text{NPV (negative predictive value)} = P(\\text{case is healthy} \\mid \\text{model predict healthy})  = \\frac{\\text{TN}}{\\text{TN} + \\text{FN}} \n",
       "\\\\ \\text{Precision} = P(\\text{case is ill} \\mid \\text{model predict ill}) = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} \n",
       "\\\\      1-\\text{Precision} = P(\\text{case is healthy} \\mid \\text{model predict ill})\n",
       "\\\\      \\text{when cost of False positive is high like false detecting a case as malignant (idle financial and stress costs for treatment)} \\qquad \\text{normal range: 0.90-0.95} \\\\\n",
       "\\\\ \\text{ }\n",
       "\\\\ \\text{ }\n",
       "\\\\ \\text{> when we know the case condition ill or healthy, senstivity (recall) and (specificity) are used}\n",
       "\\\\ \\text{Sensitivity (Recall)} = P(\\text{model predict ill} \\mid \\text{case is ill})= \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} \n",
       "\\\\      1-\\text{Recall} = P(\\text{model predict healthy} \\mid \\text{case is ill})\n",
       "\\\\      \\text{when cost of False negative is high like false detecting a case as benign (high chance of death) 1-senstivity should be low} \\qquad \\text{normal range: 0.90-0.95}\n",
       "\\\\ \\text{Specificity} = P(\\text{model predict healthy} \\mid \\text{case is healthy}) =\\frac{\\text{TN}}{\\text{TN} + \\text{FP}} \\qquad \\text{the power of model in detecting real negatives}\n",
       "\\\\ \\text{ }\n",
       "\\\\ \\text{ }\n",
       "\\\\ \\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\qquad \\text{normal range: 0.80-0.90}\n",
       "\\\\ \\text{ }\n",
       "\\\\ \\text{Balanced Accuracy} = \\frac{\\text{Recall} + \\text{Specificity}}{2} \\qquad \\text{when train data is dissymmetric}\n",
       "\\\\ \\text{ }\n",
       "\\\\ \\text{ }\n",
       "\\\\ \\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right]\n",
       "\\\\ \\text{Expected Accuracy} = \\frac{(\\text{TP} + \\text{FP}) \\times (\\text{TP} + \\text{FN}) + (\\text{TN} + \\text{FP}) \\times (\\text{TN} + \\text{FN})}{(\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN})^2}\n",
       "\\\\ \\text{Cohen's Kappa} = \\frac{\\text{Accuracy} - \\text{Expected Accuracy}}{1 - \\text{Expected Accuracy}} \\qquad \\text{the remove effect of chance}\n",
       "\\\\ \\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}, \\quad \\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\n",
       "\\\\ \\text{AUC} = \\int_{0}^{1} \\text{TPR}(FPR) \\, d(FPR)\n",
       "\\\\ \\text{MCC} = \\frac{\\text{TP} \\times \\text{TN} - \\text{FP} \\times \\text{FN}}{\\sqrt{(\\text{TP} + \\text{FP})(\\text{TP} + \\text{FN})(\\text{TN} + \\text{FP})(\\text{TN} + \\text{FN})}}\n",
       "$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Math\n",
    "formulas = r'''\n",
    "\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}} \\qquad \\text{when train data is symetric} \n",
    "\\\\ \\text{ }\n",
    "\\\\ \\text{ }\n",
    "\\\\ \\text{> when we know the model's prediction (positive) or (negative), NPV and precision are used}\n",
    "\\\\ \\text{NPV (negative predictive value)} = P(\\text{case is healthy} \\mid \\text{model predict healthy})  = \\frac{\\text{TN}}{\\text{TN} + \\text{FN}} \n",
    "\\\\ \\text{Precision} = P(\\text{case is ill} \\mid \\text{model predict ill}) = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} \n",
    "\\\\      1-\\text{Precision} = P(\\text{case is healthy} \\mid \\text{model predict ill})\n",
    "\\\\      \\text{when cost of False positive is high like false detecting a case as malignant (idle financial and stress costs for treatment)} \\qquad \\text{normal range: 0.90-0.95} \\\\\n",
    "\\\\ \\text{ }\n",
    "\\\\ \\text{ }\n",
    "\\\\ \\text{> when we know the case condition ill or healthy, senstivity (recall) and (specificity) are used}\n",
    "\\\\ \\text{Sensitivity (Recall)} = P(\\text{model predict ill} \\mid \\text{case is ill})= \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} \n",
    "\\\\      1-\\text{Recall} = P(\\text{model predict healthy} \\mid \\text{case is ill})\n",
    "\\\\      \\text{when cost of False negative is high like false detecting a case as benign (high chance of death) 1-senstivity should be low} \\qquad \\text{normal range: 0.90-0.95}\n",
    "\\\\ \\text{Specificity} = P(\\text{model predict healthy} \\mid \\text{case is healthy}) =\\frac{\\text{TN}}{\\text{TN} + \\text{FP}} \\qquad \\text{the power of model in detecting real negatives}\n",
    "\\\\ \\text{ }\n",
    "\\\\ \\text{ }\n",
    "\\\\ \\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\qquad \\text{normal range: 0.80-0.90}\n",
    "\\\\ \\text{ }\n",
    "\\\\ \\text{Balanced Accuracy} = \\frac{\\text{Recall} + \\text{Specificity}}{2} \\qquad \\text{when train data is dissymmetric}\n",
    "\\\\ \\text{ }\n",
    "\\\\ \\text{ }\n",
    "\\\\ \\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right]\n",
    "\\\\ \\text{Expected Accuracy} = \\frac{(\\text{TP} + \\text{FP}) \\times (\\text{TP} + \\text{FN}) + (\\text{TN} + \\text{FP}) \\times (\\text{TN} + \\text{FN})}{(\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN})^2}\n",
    "\\\\ \\text{Cohen's Kappa} = \\frac{\\text{Accuracy} - \\text{Expected Accuracy}}{1 - \\text{Expected Accuracy}} \\qquad \\text{the remove effect of chance}\n",
    "\\\\ \\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}, \\quad \\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\n",
    "\\\\ \\text{AUC} = \\int_{0}^{1} \\text{TPR}(FPR) \\, d(FPR)\n",
    "\\\\ \\text{MCC} = \\frac{\\text{TP} \\times \\text{TN} - \\text{FP} \\times \\text{FN}}{\\sqrt{(\\text{TP} + \\text{FP})(\\text{TP} + \\text{FN})(\\text{TN} + \\text{FP})(\\text{TN} + \\text{FN})}}\n",
    "'''\n",
    "Math(formulas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = perceptron()\n",
    "mymodel.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9868131868131869\n"
     ]
    }
   ],
   "source": [
    "print(f'train accuracy: {mymodel.accuracy}')\n",
    "y_hat_test= mymodel.pred(x_test_scaled)\n",
    "mymodel.metric(y_hat_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9298245614035088\t Precision: 0.9846153846153847\t Recall: 0.9014084507042254\t F1_score: 0.9411764705882353\t specificity:0.9767441860465116\n"
     ]
    }
   ],
   "source": [
    "print (f'accuracy: {mymodel.Accuracy_t()}\\t Precision: {mymodel.Precision()}\\t Recall: {mymodel.Recall()}\\t F1_score: {mymodel.F1_score()}\\t specificity:{mymodel.Specificity()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on metric results\n",
      "When the model detect a case as benign\n",
      "It is recommended to redo examnations\n"
     ]
    }
   ],
   "source": [
    "Precision_treshold = 0.92\n",
    "NPV_treshold = 0.95\n",
    "F1_treshold = 0.80\n",
    "print('Based on metric results')\n",
    "if mymodel.Precision() < Precision_treshold or mymodel.Recall() < Recall_treshold :\n",
    "    if mymodel.Precision() < Precision_treshold  :\n",
    "        print ('When the model detect a case as malignant')\n",
    "    elif mymodel.NPV() < NPV_treshold:\n",
    "        print ('When the model detect a case as benign')\n",
    "    print('It is recommended to redo examnations')\n",
    "if mymodel.F1_score() < F1_treshold:\n",
    "    print('Redesign your model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8571428571428571)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "healthy: 169, ill: 286, ill2healthy: 1.6923076923076923\n"
     ]
    }
   ],
   "source": [
    "num_healthies_train =len(y_train[y_train==-1])\n",
    "num_ills_train = len(y_train[y_train==1])\n",
    "ratio =num_ills_train/ num_healthies_train\n",
    "print (f'healthy: {num_healthies_train}, ill: {num_ills_train}, ill2healthy: {ratio}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
